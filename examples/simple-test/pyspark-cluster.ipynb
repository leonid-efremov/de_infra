{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": 1,
   "id": "15f28939-9541-4b12-88ed-946b4e1ca9a5",
   "metadata": {},
   "outputs": [],
   "source": [
    "import os\n",
    "from pyspark.sql import SparkSession"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "id": "d3a6d446-35eb-43df-b3e3-c3b3770cfdd9",
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "26/02/12 17:04:47 WARN NativeCodeLoader: Unable to load native-hadoop library for your platform... using builtin-java classes where applicable\n",
      "Setting default log level to \"WARN\".\n",
      "To adjust logging level use sc.setLogLevel(newLevel). For SparkR, use setLogLevel(newLevel).\n"
     ]
    }
   ],
   "source": [
    "spark = SparkSession \\\n",
    "    .builder \\\n",
    "    .appName(\"spark-test\") \\\n",
    "    .master(os.getenv('SPARK_MASTER_URL')) \\\n",
    "    .config('spark.driver.cores', '1')\\\n",
    "    .config('spark.driver.memory', '1G')\\\n",
    "    .config('spark.executor.instances', '1')\\\n",
    "    .config('spark.executor.cores', '1')\\\n",
    "    .config('spark.executor.memory', '512m')\\\n",
    "    .config(\"spark.ui.port\", \"4045\")\\\n",
    "    .config(\"spark.sql.extensions\", \"org.apache.iceberg.spark.extensions.IcebergSparkSessionExtensions\") \\\n",
    "    .config(\"spark.jars\", \"/opt/spark/jars/hadoop-aws-3.3.4.jar,/opt/spark/jars/aws-java-sdk-bundle-1.12.791.jar,/opt/spark/jars/iceberg-spark-runtime-3.5_2.12-1.10.1.jar\") \\\n",
    "    .config(\"spark.sql.catalog.iceberg\", \"org.apache.iceberg.spark.SparkCatalog\") \\\n",
    "    .config(\"spark.sql.catalog.iceberg.type\", \"nessie\") \\\n",
    "    .config(\"spark.sql.catalog.iceberg.uri\", \"http://nessie-catalog:19120/api/v2\") \\\n",
    "    .config(\"spark.sql.catalog.iceberg.ref\", \"main\") \\\n",
    "    .config(\"spark.sql.catalog.iceberg.warehouse\", \"warehouse\") \\\n",
    "    .config(\"spark.sql.catalog.iceberg.authentication.type\", \"NONE\") \\\n",
    "    .config(\"spark.sql.catalog.iceberg.s3.endpoint\", \"http://ozone-s3g:9878\") \\\n",
    "    .config(\"spark.sql.catalog.iceberg.s3.access-key-id\", \"ozone-access-key\") \\\n",
    "    .config(\"spark.sql.catalog.iceberg.s3.secret-access-key\", \"ozone-secret-key\") \\\n",
    "    .config(\"spark.sql.catalog.iceberg.s3.path-style-access\", \"true\") \\\n",
    "    .config(\"spark.sql.defaultCatalog\", \"iceberg\") \\\n",
    "    .config(\"spark.hadoop.hive.metastore.uris\", \"thrift://hive-metastore:9083\") \\\n",
    "    .config(\"spark.hadoop.fs.s3a.impl\", \"org.apache.hadoop.fs.s3a.S3AFileSystem\") \\\n",
    "    .config(\"spark.hadoop.fs.defaultFS\", \"s3a://ozone-om\") \\\n",
    "    .config(\"spark.hadoop.fs.s3a.access.key\", \"ozone-access-key\") \\\n",
    "    .config(\"spark.hadoop.fs.s3a.secret.key\", \"ozone-secret-key\") \\\n",
    "    .config(\"spark.hadoop.fs.s3a.endpoint\", \"http://ozone-s3g:9878\") \\\n",
    "    .config(\"spark.hadoop.fs.s3a.path.style.access\", \"true\") \\\n",
    "    .config(\"spark.hadoop.fs.s3a.connection.ssl.enabled\", \"false\") \\\n",
    "    .config(\"spark.hadoop.fs.s3a.committer.name\", \"directory\") \\\n",
    "    .config(\"spark.hadoop.fs.s3a.committer.staging.tmp.path\", \"/tmp/staging\") \\\n",
    "    .config(\"spark.hadoop.fs.s3a.multiobjectdelete.enable\", \"false\") \\\n",
    "    .config(\"spark.hadoop.fs.s3a.directory.marker.retention\", \"keep\") \\\n",
    "    .config(\"spark.hadoop.fs.s3a.connection.establish.timeout\", \"5000\") \\\n",
    "    .config(\"spark.hadoop.fs.s3a.connection.timeout\", \"60000\") \\\n",
    "    .config(\"spark.hadoop.fs.s3a.socket.timeout\", \"60000\") \\\n",
    "    .config(\"spark.hadoop.fs.s3a.attempts.maximum\", \"50\") \\\n",
    "    .config(\"spark.hadoop.fs.s3a.retry.limit\", \"50\") \\\n",
    "    .config(\"spark.hadoop.fs.s3a.retry.interval\", \"5s\") \\\n",
    "    .getOrCreate()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "id": "7c58ff1b-2963-49e6-81a8-ae2b566c6050",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "+-------------+\n",
      "|      catalog|\n",
      "+-------------+\n",
      "|      iceberg|\n",
      "|spark_catalog|\n",
      "+-------------+\n",
      "\n"
     ]
    }
   ],
   "source": [
    "spark.sql(\"SHOW CATALOGS;\").show()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "id": "d7d3f3d6-227b-4bab-8e1a-6b2491912bb9",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "DataFrame[]"
      ]
     },
     "execution_count": 4,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "spark.sql(\"CREATE DATABASE IF NOT EXISTS iceberg.test_iceberg_db_2 LOCATION 's3a://mybucket/warehouse/test_iceberg_db_2'\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "id": "cf6524a2-908c-40d7-b465-17a9b7c3b652",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "+-----------------+\n",
      "|        namespace|\n",
      "+-----------------+\n",
      "|test_iceberg_db_2|\n",
      "|  test_iceberg_db|\n",
      "+-----------------+\n",
      "\n"
     ]
    }
   ],
   "source": [
    "spark.sql(\"SHOW DATABASES IN iceberg\").show()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 10,
   "id": "bb540111-7b44-4b70-9f53-48ad1efb8ff9",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "+-----------------+------------------+-----------+\n",
      "|        namespace|         tableName|isTemporary|\n",
      "+-----------------+------------------+-----------+\n",
      "|test_iceberg_db_2|test_iceberg_table|      false|\n",
      "+-----------------+------------------+-----------+\n",
      "\n"
     ]
    }
   ],
   "source": [
    "spark.sql(\"SHOW TABLES IN iceberg.test_iceberg_db_2\").show()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 7,
   "id": "3dc71731-3c79-43c2-8764-b78580cb7d6d",
   "metadata": {},
   "outputs": [],
   "source": [
    "df = spark.createDataFrame([(1, \"tgete\", \"2025-02-01\"), (3, \"vdsv\", \"2025-06-01\")], [\"id\", \"descr\", \"report_dt\"])"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 8,
   "id": "2a8aa092-4351-4a0d-ab45-d655552ca95f",
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "                                                                                \r"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "+---+-----+----------+\n",
      "| id|descr| report_dt|\n",
      "+---+-----+----------+\n",
      "|  1|tgete|2025-02-01|\n",
      "|  3| vdsv|2025-06-01|\n",
      "+---+-----+----------+\n",
      "\n"
     ]
    }
   ],
   "source": [
    "df.show()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 9,
   "id": "b40cfa91-27ef-451d-97ca-4b8782e5763c",
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "26/02/12 17:07:02 WARN MetricsConfig: Cannot locate configuration: tried hadoop-metrics2-s3a-file-system.properties,hadoop-metrics2.properties\n",
      "26/02/12 17:07:02 WARN VersionInfoUtils: The AWS SDK for Java 1.x entered maintenance mode starting July 31, 2024 and will reach end of support on December 31, 2025. For more information, see https://aws.amazon.com/blogs/developer/the-aws-sdk-for-java-1-x-is-in-maintenance-mode-effective-july-31-2024/\n",
      "You can print where on the file system the AWS SDK for Java 1.x core runtime is located by setting the AWS_JAVA_V1_PRINT_LOCATION environment variable or aws.java.v1.printLocation system property to 'true'.\n",
      "This message can be disabled by setting the AWS_JAVA_V1_DISABLE_DEPRECATION_ANNOUNCEMENT environment variable or aws.java.v1.disableDeprecationAnnouncement system property to 'true'.\n",
      "The AWS SDK for Java 1.x is being used here:\n",
      "at java.base/java.lang.Thread.getStackTrace(Thread.java:1619)\n",
      "at com.amazonaws.util.VersionInfoUtils.printDeprecationAnnouncement(VersionInfoUtils.java:81)\n",
      "at com.amazonaws.util.VersionInfoUtils.<clinit>(VersionInfoUtils.java:59)\n",
      "at com.amazonaws.internal.EC2ResourceFetcher.<clinit>(EC2ResourceFetcher.java:44)\n",
      "at com.amazonaws.auth.InstanceMetadataServiceCredentialsFetcher.<init>(InstanceMetadataServiceCredentialsFetcher.java:38)\n",
      "at com.amazonaws.auth.InstanceProfileCredentialsProvider.<init>(InstanceProfileCredentialsProvider.java:111)\n",
      "at com.amazonaws.auth.InstanceProfileCredentialsProvider.<init>(InstanceProfileCredentialsProvider.java:91)\n",
      "at com.amazonaws.auth.InstanceProfileCredentialsProvider.<init>(InstanceProfileCredentialsProvider.java:75)\n",
      "at com.amazonaws.auth.InstanceProfileCredentialsProvider.<clinit>(InstanceProfileCredentialsProvider.java:58)\n",
      "at com.amazonaws.auth.EC2ContainerCredentialsProviderWrapper.initializeProvider(EC2ContainerCredentialsProviderWrapper.java:66)\n",
      "at com.amazonaws.auth.EC2ContainerCredentialsProviderWrapper.<init>(EC2ContainerCredentialsProviderWrapper.java:55)\n",
      "at org.apache.hadoop.fs.s3a.auth.IAMInstanceCredentialsProvider.<init>(IAMInstanceCredentialsProvider.java:49)\n",
      "at java.base/jdk.internal.reflect.NativeConstructorAccessorImpl.newInstance0(Native Method)\n",
      "at java.base/jdk.internal.reflect.NativeConstructorAccessorImpl.newInstance(NativeConstructorAccessorImpl.java:77)\n",
      "at java.base/jdk.internal.reflect.DelegatingConstructorAccessorImpl.newInstance(DelegatingConstructorAccessorImpl.java:45)\n",
      "at java.base/java.lang.reflect.Constructor.newInstanceWithCaller(Constructor.java:500)\n",
      "at java.base/java.lang.reflect.Constructor.newInstance(Constructor.java:481)\n",
      "at org.apache.hadoop.fs.s3a.S3AUtils.createAWSCredentialProvider(S3AUtils.java:766)\n",
      "at org.apache.hadoop.fs.s3a.S3AUtils.buildAWSProviderList(S3AUtils.java:698)\n",
      "at org.apache.hadoop.fs.s3a.S3AUtils.createAWSCredentialProviderSet(S3AUtils.java:631)\n",
      "at org.apache.hadoop.fs.s3a.S3AFileSystem.bindAWSClient(S3AFileSystem.java:877)\n",
      "at org.apache.hadoop.fs.s3a.S3AFileSystem.initialize(S3AFileSystem.java:534)\n",
      "at org.apache.hadoop.fs.FileSystem.createFileSystem(FileSystem.java:3469)\n",
      "at org.apache.hadoop.fs.FileSystem.access$300(FileSystem.java:174)\n",
      "at org.apache.hadoop.fs.FileSystem$Cache.getInternal(FileSystem.java:3574)\n",
      "at org.apache.hadoop.fs.FileSystem$Cache.get(FileSystem.java:3521)\n",
      "at org.apache.hadoop.fs.FileSystem.get(FileSystem.java:540)\n",
      "at org.apache.hadoop.fs.Path.getFileSystem(Path.java:365)\n",
      "at org.apache.iceberg.hadoop.Util.getFs(Util.java:55)\n",
      "at org.apache.iceberg.hadoop.HadoopOutputFile.fromPath(HadoopOutputFile.java:53)\n",
      "at org.apache.iceberg.hadoop.HadoopFileIO.newOutputFile(HadoopFileIO.java:97)\n",
      "at org.apache.iceberg.BaseMetastoreTableOperations.writeNewMetadata(BaseMetastoreTableOperations.java:156)\n",
      "at org.apache.iceberg.BaseMetastoreTableOperations.writeNewMetadataIfRequired(BaseMetastoreTableOperations.java:151)\n",
      "at org.apache.iceberg.nessie.NessieTableOperations.doCommit(NessieTableOperations.java:115)\n",
      "at org.apache.iceberg.BaseMetastoreTableOperations.commit(BaseMetastoreTableOperations.java:126)\n",
      "at org.apache.iceberg.BaseMetastoreCatalog$BaseMetastoreCatalogTableBuilder.create(BaseMetastoreCatalog.java:201)\n",
      "at org.apache.iceberg.CachingCatalog$CachingTableBuilder.lambda$create$0(CachingCatalog.java:264)\n",
      "at org.apache.iceberg.shaded.com.github.benmanes.caffeine.cache.BoundedLocalCache.lambda$doComputeIfAbsent$14(BoundedLocalCache.java:2406)\n",
      "at java.base/java.util.concurrent.ConcurrentHashMap.compute(ConcurrentHashMap.java:1916)\n",
      "at org.apache.iceberg.shaded.com.github.benmanes.caffeine.cache.BoundedLocalCache.doComputeIfAbsent(BoundedLocalCache.java:2404)\n",
      "at org.apache.iceberg.shaded.com.github.benmanes.caffeine.cache.BoundedLocalCache.computeIfAbsent(BoundedLocalCache.java:2387)\n",
      "at org.apache.iceberg.shaded.com.github.benmanes.caffeine.cache.LocalCache.computeIfAbsent(LocalCache.java:108)\n",
      "at org.apache.iceberg.shaded.com.github.benmanes.caffeine.cache.LocalManualCache.get(LocalManualCache.java:62)\n",
      "at org.apache.iceberg.CachingCatalog$CachingTableBuilder.create(CachingCatalog.java:260)\n",
      "at org.apache.iceberg.spark.SparkCatalog.createTable(SparkCatalog.java:248)\n",
      "at org.apache.spark.sql.connector.catalog.TableCatalog.createTable(TableCatalog.java:223)\n",
      "at org.apache.spark.sql.execution.datasources.v2.CreateTableExec.run(CreateTableExec.scala:44)\n",
      "at org.apache.spark.sql.execution.datasources.v2.V2CommandExec.result$lzycompute(V2CommandExec.scala:43)\n",
      "at org.apache.spark.sql.execution.datasources.v2.V2CommandExec.result(V2CommandExec.scala:43)\n",
      "at org.apache.spark.sql.execution.datasources.v2.V2CommandExec.executeCollect(V2CommandExec.scala:49)\n",
      "at org.apache.spark.sql.execution.QueryExecution$$anonfun$eagerlyExecuteCommands$1.$anonfun$applyOrElse$1(QueryExecution.scala:107)\n",
      "at org.apache.spark.sql.execution.SQLExecution$.$anonfun$withNewExecutionId$6(SQLExecution.scala:125)\n",
      "at org.apache.spark.sql.execution.SQLExecution$.withSQLConfPropagated(SQLExecution.scala:201)\n",
      "at org.apache.spark.sql.execution.SQLExecution$.$anonfun$withNewExecutionId$1(SQLExecution.scala:108)\n",
      "at org.apache.spark.sql.SparkSession.withActive(SparkSession.scala:900)\n",
      "at org.apache.spark.sql.execution.SQLExecution$.withNewExecutionId(SQLExecution.scala:66)\n",
      "at org.apache.spark.sql.execution.QueryExecution$$anonfun$eagerlyExecuteCommands$1.applyOrElse(QueryExecution.scala:107)\n",
      "at org.apache.spark.sql.execution.QueryExecution$$anonfun$eagerlyExecuteCommands$1.applyOrElse(QueryExecution.scala:98)\n",
      "at org.apache.spark.sql.catalyst.trees.TreeNode.$anonfun$transformDownWithPruning$1(TreeNode.scala:461)\n",
      "at org.apache.spark.sql.catalyst.trees.CurrentOrigin$.withOrigin(origin.scala:76)\n",
      "at org.apache.spark.sql.catalyst.trees.TreeNode.transformDownWithPruning(TreeNode.scala:461)\n",
      "at org.apache.spark.sql.catalyst.plans.logical.LogicalPlan.org$apache$spark$sql$catalyst$plans$logical$AnalysisHelper$$super$transformDownWithPruning(LogicalPlan.scala:32)\n",
      "at org.apache.spark.sql.catalyst.plans.logical.AnalysisHelper.transformDownWithPruning(AnalysisHelper.scala:267)\n",
      "at org.apache.spark.sql.catalyst.plans.logical.AnalysisHelper.transformDownWithPruning$(AnalysisHelper.scala:263)\n",
      "at org.apache.spark.sql.catalyst.plans.logical.LogicalPlan.transformDownWithPruning(LogicalPlan.scala:32)\n",
      "at org.apache.spark.sql.catalyst.plans.logical.LogicalPlan.transformDownWithPruning(LogicalPlan.scala:32)\n",
      "at org.apache.spark.sql.catalyst.trees.TreeNode.transformDown(TreeNode.scala:437)\n",
      "at org.apache.spark.sql.execution.QueryExecution.eagerlyExecuteCommands(QueryExecution.scala:98)\n",
      "at org.apache.spark.sql.execution.QueryExecution.commandExecuted$lzycompute(QueryExecution.scala:85)\n",
      "at org.apache.spark.sql.execution.QueryExecution.commandExecuted(QueryExecution.scala:83)\n",
      "at org.apache.spark.sql.Dataset.<init>(Dataset.scala:220)\n",
      "at org.apache.spark.sql.Dataset$.$anonfun$ofRows$2(Dataset.scala:100)\n",
      "at org.apache.spark.sql.SparkSession.withActive(SparkSession.scala:900)\n",
      "at org.apache.spark.sql.Dataset$.ofRows(Dataset.scala:97)\n",
      "at org.apache.spark.sql.SparkSession.$anonfun$sql$1(SparkSession.scala:638)\n",
      "at org.apache.spark.sql.SparkSession.withActive(SparkSession.scala:900)\n",
      "at org.apache.spark.sql.SparkSession.sql(SparkSession.scala:629)\n",
      "at org.apache.spark.sql.SparkSession.sql(SparkSession.scala:659)\n",
      "at java.base/jdk.internal.reflect.NativeMethodAccessorImpl.invoke0(Native Method)\n",
      "at java.base/jdk.internal.reflect.NativeMethodAccessorImpl.invoke(NativeMethodAccessorImpl.java:77)\n",
      "at java.base/jdk.internal.reflect.DelegatingMethodAccessorImpl.invoke(DelegatingMethodAccessorImpl.java:43)\n",
      "at java.base/java.lang.reflect.Method.invoke(Method.java:569)\n",
      "at py4j.reflection.MethodInvoker.invoke(MethodInvoker.java:244)\n",
      "at py4j.reflection.ReflectionEngine.invoke(ReflectionEngine.java:374)\n",
      "at py4j.Gateway.invoke(Gateway.java:282)\n",
      "at py4j.commands.AbstractCommand.invokeMethod(AbstractCommand.java:132)\n",
      "at py4j.commands.CallCommand.execute(CallCommand.java:79)\n",
      "at py4j.ClientServerConnection.waitForCommands(ClientServerConnection.java:182)\n",
      "at py4j.ClientServerConnection.run(ClientServerConnection.java:106)\n",
      "at java.base/java.lang.Thread.run(Thread.java:840)\n"
     ]
    },
    {
     "data": {
      "text/plain": [
       "DataFrame[]"
      ]
     },
     "execution_count": 9,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "spark.sql(\"\"\"\n",
    "CREATE TABLE IF NOT EXISTS iceberg.test_iceberg_db_2.test_iceberg_table (\n",
    "    id INT,\n",
    "    descr STRING,\n",
    "    report_dt STRING\n",
    ") USING iceberg\n",
    "PARTITIONED BY (report_dt)\n",
    "\"\"\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 12,
   "id": "6a3e8bdb-c0d9-4b45-ad4e-34d7ef968411",
   "metadata": {},
   "outputs": [],
   "source": [
    "# insert into table\n",
    "df.write\\\n",
    "    .mode(\"append\") \\\n",
    "    .insertInto(\"iceberg.test_iceberg_db_2.test_iceberg_table\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 13,
   "id": "90fa82b1-3bf8-4e25-8c6f-c218b1d37973",
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "[Stage 8:>                                                          (0 + 1) / 1]\r"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "+---+-----+----------+\n",
      "| id|descr| report_dt|\n",
      "+---+-----+----------+\n",
      "|  1|tgete|2025-02-01|\n",
      "|  1|tgete|2025-02-01|\n",
      "|  3| vdsv|2025-06-01|\n",
      "|  3| vdsv|2025-06-01|\n",
      "+---+-----+----------+\n",
      "\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "                                                                                \r"
     ]
    }
   ],
   "source": [
    "spark \\\n",
    "    .table(\"iceberg.test_iceberg_db_2.test_iceberg_table\") \\\n",
    "    .show()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 14,
   "id": "fdddb5b7-9114-445a-9e7f-49bd795c4138",
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "                                                                                \r"
     ]
    }
   ],
   "source": [
    "# write file to S3\n",
    "df.write \\\n",
    "    .mode(\"overwrite\") \\\n",
    "    .format(\"parquet\") \\\n",
    "    .save(\"s3a://mybucket/test_db/spark_test/test.parquet\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 15,
   "id": "2419b295-4733-4e31-a3b6-73c8be7658ac",
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "                                                                                \r"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "+---+-----+----------+\n",
      "| id|descr| report_dt|\n",
      "+---+-----+----------+\n",
      "|  1|tgete|2025-02-01|\n",
      "|  3| vdsv|2025-06-01|\n",
      "+---+-----+----------+\n",
      "\n"
     ]
    }
   ],
   "source": [
    "spark.read \\\n",
    "    .parquet(\"s3a://mybucket/test_db/spark_test/test.parquet\") \\\n",
    "    .show()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 16,
   "id": "839810dd-7e11-4df6-8936-bf871c429a99",
   "metadata": {},
   "outputs": [],
   "source": [
    "spark.stop()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "5e45ac98-ce04-4cff-be7a-d3bf3d581b5c",
   "metadata": {},
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3 (ipykernel)",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.10.11"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 5
}
